`torch.normal(a,b,(c,d))`从以a为平均值，b为方差的正态分布中随机抽取c*d个数组成一个形状为（c,d）的张量返回

`yield a,b `返回一个迭代器，使用for X,y in func()就能迭代访问返回的所有a,b

`with torch.no_grad():`语句块中的操作不会被记录在计算图中，这在深度学习中是非常重要的。计算图是用于自动求导的核心机制，它记录了每个操作的依赖关系，从而可以通过反向传播计算梯度。在训练过程中，我们通常需要计算参数的梯度并更新参数，但有时候我们希望某些操作不被计算图记录，例如在更新参数时不需要记录梯度的计算。

`zip([1,2],[a,b])`将两个列表按次序构成元组列表返回[(1,a),(2,b)]

`y.numel()`方法返回张量中元素的总数，即当前批次数据的样本数量